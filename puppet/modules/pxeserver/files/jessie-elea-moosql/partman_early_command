#!/bin/sh

### This file is managed by Puppet, don't edit it ###

exec >/tmp/partition.log 2>&1
set -x

parted='parted --script --align=opt'

c_ssd=1
c_sata=1
for i in a b c d
do
    size=$(parted -m /dev/sd$i unit GB print | grep "^/dev/sd$i" | cut -d':' -f2)
    if [ "$size" = '200GB' ]
    then
        eval l_ssd$c_ssd=$i
        c_ssd=$((c_ssd + 1))
    else
        eval l_sata$c_sata=$i
        c_sata=$((c_sata + 1))
    fi
done

# Now, we have:
#
#   - $l_ssd1 and $l_ssd2 the letters of SSD.
#   - $l_sata1 and $l_sata2 the letters of SATA.


### Remove the RAID volumes if already created. ###

i='3' # on /dev/sd${l_sata1}3 and /dev/sd${l_sata2}3
raid1system='md0'
[ -e /dev/$raid1system ]   && mdadm --stop /dev/$raid1system
[ -e /dev/sd${l_sata1}$i ] && mdadm --zero-superblock /dev/sd${l_sata1}$i
[ -e /dev/sd${l_sata2}$i ] && mdadm --zero-superblock /dev/sd${l_sata2}$i

i='4' # on /dev/sd${l_sata1}4 and /dev/sd${l_sata2}4
raid1swap='md1'
[ -e /dev/$raid1swap ]     && mdadm --stop /dev/$raid1swap
[ -e /dev/sd${l_sata1}$i ] && mdadm --zero-superblock /dev/sd${l_sata1}$i
[ -e /dev/sd${l_sata2}$i ] && mdadm --zero-superblock /dev/sd${l_sata2}$i

i='5' # on /dev/sd${l_sata1}5 and /dev/sd${l_sata2}5
raid1hd='md2'
[ -e /dev/$raid1hd ]       && mdadm --stop /dev/$raid1hd
[ -e /dev/sd${l_sata1}$i ] && mdadm --zero-superblock /dev/sd${l_sata1}$i
[ -e /dev/sd${l_sata2}$i ] && mdadm --zero-superblock /dev/sd${l_sata2}$i

i='1' # on /dev/sd${l_ssd1}1 and /dev/sd${l_ssd2}1
raid1ssd='md3'
[ -e /dev/$raid1ssd ]     && mdadm --stop /dev/$raid1ssd
[ -e /dev/sd${l_ssd1}$i ] && mdadm --zero-superblock /dev/sd${l_ssd1}$i
[ -e /dev/sd${l_ssd2}$i ] && mdadm --zero-superblock /dev/sd${l_ssd2}$i


### Create GPT partition on each disk. ###
for i in a b c d
do
    $parted /dev/sd${i} mktable gpt
done


### Partitioning on the non-SSD drives. ###
n=0
for i in ${l_sata1} ${l_sata2}
do
    # a => n = 1, b => n = 2.
    n=$((n + 1))

    part_num=1

    # The unused UEFI partitions if one day we decide
    # to enable the Bios-UEFI.
    a=1
    b=$((250 + a)) # Size == 250MiB
    $parted /dev/sd${i} -- unit MiB mkpart uefi${n}unused $a $b
    part_num=$((part_num + 1))

    # The biosgrub partitions.
    a=$b
    b=$((1 + a)) # Size == 1MiB
    $parted /dev/sd${i} -- unit MiB mkpart biosgrub${n} $a $b
    $parted /dev/sd${i} set $part_num bios_grub on
    part_num=$((part_num + 1))

    # The root partitions (will be a RAID1 volume).
    a=$b
    b=$((30 * 1024 + a)) # Size == 30GiB
    $parted /dev/sd${i} -- unit MiB mkpart system${n} $a $b
    $parted /dev/sd${i} set $part_num raid on
    part_num=$((part_num + 1))

    # The swap (will be a RAID1 volume).
    a=$b
    b=$((8 * 1024 + a)) # Size == 8GiB
    $parted /dev/sd${i} -- unit MiB mkpart swap${n} linux-swap $a $b
    $parted /dev/sd${i} set $part_num raid on
    part_num=$((part_num + 1))

    # The remaining of the disk is a LVM partition in a RAID1 volume.
    a=$b
    b='-1cyl' # The last cylinder
    $parted /dev/sd${i} -- unit MiB mkpart lvm-hd${n} $a $b
    $parted /dev/sd${i} set $part_num raid on
    part_num=$((part_num + 1))

done


### Partitioning on the SSD drives. ###
n=0
for i in ${l_ssd1} ${l_ssd2}
do
    # c => n = 1, d => n = 2.
    n=$((n + 1))

    # The LVM partition on the RAID1 volume in the two SSD.
    a=1
    b='-1cyl' # The last cylinder
    $parted /dev/sd${i} -- unit MiB mkpart lvm-ssd${n} $a $b
    $parted /dev/sd${i} set 1 raid on
done


### Creation of the RAID volumes. ###

# The system (/) partition.
mdadm --create /dev/$raid1system --level=1 --raid-devices=2 /dev/sd${l_sata1}3 /dev/sd${l_sata2}3 --force --run

# The swap partition.
mdadm --create /dev/$raid1swap --level=1 --raid-devices=2 /dev/sd${l_sata1}4 /dev/sd${l_sata2}4 --force --run

# The LVM partition in the harddrive.
mdadm --create /dev/$raid1hd --level=1 --raid-devices=2 /dev/sd${l_sata1}5 /dev/sd${l_sata2}5 --force --run

# The SSD RAID1 volume.
mdadm --create /dev/$raid1ssd --level=1 --raid-devices=2 /dev/sd${l_ssd1}1 /dev/sd${l_ssd2}1 --force --run


### Creation of the volume group LVM on the HD RAID1 volume etc. ###
pvcreate -ff --yes /dev/$raid1hd
vgcreate --force --yes vg1 /dev/$raid1hd
lvcreate --name varlogmysql --size 200g vg1
lvcreate --name backups     --size 500g vg1

### Creation of the volume group LVM on the SSD RAID1 volume etc. ###
pvcreate -ff --yes /dev/$raid1ssd
vgcreate --force --yes vg2 /dev/$raid1ssd
lvcreate --name tmp         --size 30g  vg2
lvcreate --name varlibmysql --size 120g vg2


### Creation of the file systems. ###
mkfs.ext4 -F -E lazy_itable_init=0 -L system /dev/$raid1system
mkswap -L swap /dev/$raid1swap
mkfs.xfs -f -L varlogmysql /dev/mapper/vg1-varlogmysql
mkfs.xfs -f -L backups     /dev/mapper/vg1-backups
mkfs.ext4 -F -E lazy_itable_init=0 -L tmp         /dev/mapper/vg2-tmp
mkfs.ext4 -F -E lazy_itable_init=0 -L varlibmysql /dev/mapper/vg2-varlibmysql

# mkfs.vfat doesn't exist during Debian installation.
# Lowercase labels trigger a warning.
mkfs.fat -F 32 -n 'UEFI1' /dev/sd${l_sata1}1 # partition unused but hey...
mkfs.fat -F 32 -n 'UEFI2' /dev/sd${l_sata2}1 # partition unused but hey...

exit 0


