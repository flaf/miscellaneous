#!/bin/sh

Ceph_ready
Ceph_install

mon="$1"
cluster="$2"
fs="$3"
device="$4"

Ceph_pick_conf "$mon" "$cluster"

# UUID of the osd.
uuid=$(uuidgen)

# Create the OSD. The command will output the OSD number,
# which we will need for subsequent steps.
osd_id=$(ceph --cluster "$cluster" osd create "$uuid")
printf "The id of this osd will be $osd_id.\n"

# Creation of the osd data directory.
mkdir "/var/lib/ceph/osd/$cluster-$osd_id/"
mount -t "$fs" "$device" "/var/lib/ceph/osd/$cluster-$osd_id/"
rm -rf "/var/lib/ceph/osd/$cluster-$osd_id/"* # cleaning of the partition.

# Initialize the osd data directory. The keyring is generated
# with the --mkkey option. Cluster fsid and list of monitors
# are provided in the $cluster.conf file.
ceph-osd -i "$osd_id" --mkfs --mkkey --cluster "$cluster"     \
          --conf "/etc/ceph/$cluster.conf" --osd-uuid "$uuid" \
          -m "$mon" >/dev/null 2>&1

# Register the OSD authentication key.
ceph auth add "osd.$osd_id" osd 'allow *' mon 'allow profile osd' \
     -i "/var/lib/ceph/osd/$cluster-$osd_id/keyring"              \
     --cluster "$cluster" --conf "/etc/ceph/$cluster.conf"        \
     -m "$mon"

# Add your Ceph Node to the CRUSH map.
ceph osd crush add-bucket $(hostname) host --cluster "$cluster" -m "$mon"

# Place the Ceph Node under the root default.
ceph osd crush move $(hostname) root=default --cluster "$cluster" -m "$mon"

# Add the osd to the CRUSH map so that it can begin receiving data.
# 1.0 is the weight of the osd.
ceph osd crush add "osd.$osd_id" 1.0 host=$(hostname) --cluster "$cluster" -m "$mon"

# To allow the start of the daemon at each reboot.
touch "/var/lib/ceph/osd/$cluster-$osd_id/ready"
touch "/var/lib/ceph/osd/$cluster-$osd_id/upstart"

# Restart of the daemon.
stop ceph-osd-all
start ceph-osd-all

# Define UUID variable of the device (the UUID partition in fact).
eval $(blkid | grep ^$device | sed -r 's/.*(UUID=[^ ]*).*/\1/')


printf "Update of /etc/fstab.\n"
printf "\n# osd storage.\n" >> "/etc/fstab"
printf "UUID=$UUID /var/lib/ceph/osd/$cluster-$osd_id/ $fs defaults,noatime 0 2\n\n" >> "/etc/fstab"


# To check the result:
#
#   ceph osd tree --cluster "$cluster"
#   ceph status --cluster "$cluster
#



