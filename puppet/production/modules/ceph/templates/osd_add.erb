#!/bin/sh
### This is a file managed by Puppet, don't edit it. ###
<%-
    cluster_name         = scope['::ceph::cluster_name']
    monitors             = scope['::ceph::monitors']
    monitor_init         = scope['::ceph::monitor_init']
    monitor_init_address = monitors[monitor_init]['address']
-%>

# The first argument is the partition used by the osd
# (ie /dev/sdb1 etc).
partition="$1"

# The second argument is the mount options. For instance
# "defaults,noatime".
mount_options="$2"

# The name of the cluster.
cluster='<%= cluster_name %>'

# The address of the initial monitor.
monitor_init_address='<%= monitor_init_address %>'


SCRIPT_NAME=${0##*/}
export LC_ALL=C
export PATH='/usr/sbin:/usr/bin:/sbin:/bin'

# Creation of an osd. The osd ID will be generated
# automatically.
osd_id=$(ceph --cluster "$cluster" -m "$monitor_init_address" \
         osd create "$uuid")
printf "The id of this osd will be $osd_id.\n"

# Create the osd working directory.
if [ -e "/var/lib/ceph/osd/$cluster-$osd_id" ]
then
    printf "Sorry, the file (or directory etc.) "
    printf "/var/lib/ceph/osd/$cluster-$osd_id already exists.\n"
    printf "End of the script.\n"
    exit 1
fi

if ! mkdir "/var/lib/ceph/osd/$cluster-$osd_id/"
then
    printf "Sorry, impossible to create the "
    printf "/var/lib/ceph/osd/$cluster-$osd_id/ directory.\n"
    printf "End of the script.\n"
    exit 1
fi

# Get the UUID and the filesystem type of the partition.
UUID=''
TYPE=''
eval $(blkid -o export "$partition")

if [ -z "$UUID" ]
then
    printf "Sorry, no UUID found for the $partition.\n"
    printf "End of the script.\n"
    exit 1
fi

if [ -z "$TYPE" ]
then
    printf "Sorry, no filesystem type found for the $partition.\n"
    printf "End of the script.\n"
    exit 1
fi

if grep -q "/var/lib/ceph/osd/$cluster-$osd_id/" /etc/fstab
then
    printf "Sorry, the mountpoint /var/lib/ceph/osd/$cluster-$osd_id/ "
    printf "already exists in /etc/fstab.\n"
    printf "End of the script.\n"
    exit 1
fi

if grep -q "UUID=$UUID" /etc/fstab
then
    printf "Sorry, a partition with the UUID=$UUID "
    printf "already exists in /etc/fstab.\n"
    printf "End of the script.\n"
    exit 1
fi

if mount | grep -q "^$partition"
then
    printf "Sorry, the partition $partition seems to be "
    printf "already mounted.\n"
    printf "End of the script.\n"
    exit 1
fi

printf "Update of /etc/fstab...\n"

printf "\n# OSD storage.\n" >> /etc/fstab
printf "UUID=$UUID /var/lib/ceph/osd/$cluster-$osd_id/ $TYPE $mount_options 0 2\n\n" \
    >> /etc/fstab

if ! mount "/var/lib/ceph/osd/$cluster-$osd_id/"
then
    printf "Sorry, impossible to mount $partition.\n"
    printf "End of the script.\n"
    exit 1
fi

printf "Here is the content of $partition mounted "
printf "on /var/lib/ceph/osd/$cluster-$osd_id/:\n\n"

ls -al "/var/lib/ceph/osd/$cluster-$osd_id/"

printf "\nNormally, it should be empty. This content will be modified "
printf "during the initialization of the OSD working directory.\n"
printf "Do you want to continue (either \"yes\" or whatever)? "

read continue

if [ "$continue" != 'yes' ]
then
    printf "Ok, end of the script.\n"
    exit 0
fi

# Each osd has its owned uuid.
uuid=$(uuidgen)

# Initialization of the osd working directory. The --mkkey
# generates automatically the keyring of this osd. The fsid
# and the monitors list are provided in the /etc/ceph/$cluster.conf
# file.
# Normally, this command displays curious messages which
# looks like error message but this is not the case.
ceph-osd -i "$osd_id" --mkfs --mkkey --cluster "$cluster" \
    --conf "/etc/ceph/$cluster.conf" --osd-uuid "$uuid"

# Recording in the cluster of the osd keyring.
ceph auth add "osd.$osd_id" osd 'allow *' mon 'allow profile osd' \
    -i "/var/lib/ceph/osd/$cluster-$osd_id/keyring"               \
    -m "$monitor_init_address"                                    \
    --cluster "$cluster" --conf "/etc/ceph/$cluster.conf"

# Add the the current host in the CRUSH map.
# If the host is already in the CRUSH map, the command
# prints a message and returns 0.
ceph osd crush add-bucket $(hostname) host --cluster "$cluster" \
    -m "$monitor_init_address"

# Put the current host in the "default" root of the CRUSH
# map. If the host is already in the "default" root, the
# command prints a message and returns 0.
ceph osd crush move $(hostname) root=default --cluster "$cluster" \
    -m "$monitor_init_address"

# Declare in the cluster the osd with the weight equal to 1.
ceph osd crush add "osd.$osd_id" 1.0 host=$(hostname) \
    --cluster "$cluster" -m "$monitor_init_address"

# Without these files, the osd daemon doesn't start.
touch "/var/lib/ceph/osd/$cluster-$osd_id/ready"
touch "/var/lib/ceph/osd/$cluster-$osd_id/upstart"

# Start the osd daemon.
stop ceph-osd cluster="$cluster" id="$osd_id"
sleep 0.5
start ceph-osd cluster="$cluster" id="$osd_id"


